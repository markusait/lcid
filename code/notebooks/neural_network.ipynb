{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "\n",
    "We create a neural network with only **70 lines of code** (orignal code can be found in network.py).\n",
    "Notice how we only import numpy (for better matrix operations) and dont use any other third-party libaries.\n",
    "\n",
    "Fig 1:\n",
    "![Example Neural Network](http://neuralnetworksanddeeplearning.com/images/tikz12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 87)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m87\u001b[0m\n\u001b[0;31m    for b, nb in zip(self.biases, nabla_b)]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, biases=None, weights=None,):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\n",
    "        For a [2, 3, 1] network we get a [len3,len1] array for biases\n",
    "        And for weights we get a [len3[len2],len3] array.\n",
    "        Think of incoming connections!\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = biases if biases != None else [\n",
    "            np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = weights if weights != None else [np.random.randn(y, x)\n",
    "                                                      for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "        classification_rates = []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                classification_rates.append(self.evaluate(test_data) / n_test)\n",
    "                print(f\"Epoch {j} : {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    ")\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # list to store all the activations, layer by layer\n",
    "        zs = []  # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute the load in 60.000 handwritten digits from the MINST Database as our training data.\n",
    "Our training data has the shape of a list of tuples like such: (28x28 pixel image, digit values from 0-9) mnist_loader.py goes more in depth on the data.\n",
    "\n",
    "Here's a few images from MNIST: ![images](http://neuralnetworksanddeeplearning.com/images/digits_separate.png)\n",
    "\n",
    "As shown in Fig.1, for the structure of our neurons we choose a *781x30x10* network. 781 because of the 28x28 = 781 input image, 10 for the result numbers and 30 without any particular reason. \n",
    "\n",
    "\n",
    "After each epoch we test the accuray of our network with a set of 10.000 training data. One can adjust the given parameters further. I am using matplotlip to plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import hack, see: https://stackoverflow.com/questions/6323860/sibling-package-imports\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from neuralnets import mnist_loader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Getting the training and test data\n",
    "training_data, _, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# You can read more about the shape of the mnist image data in mnist_loader \n",
    "# But basically we the ``training_data`` is as a tuple with two entries.\n",
    "# The first entry contains the actual training images in form of a 28 x 28 numpy ndarray.\n",
    "# The second entry is a numpy ndarray with just the digit values (0...9) for the corresponding images \n",
    "\n",
    "training_data_list = list(training_data)\n",
    "first_image = training_data_list[0][0]\n",
    "first_number = training_data_list[0][1]\n",
    "\n",
    "print(\"first_number as a 1x10 vector\", first_number)\n",
    "\n",
    "# Converting the image to its original shape as we transformed it to a 1x787 vector \n",
    "# for the input layer\n",
    "image = np.array(first_image).reshape(28,28)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hack, see: https://stackoverflow.com/questions/6323860/sibling-package-imports\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from neuralnets import mnist_loader\n",
    "# Getting the training and test data\n",
    "training_data, _, test_data = mnist_loader.load_data_wrapper()\n",
    "# Constructing the network\n",
    "net = Network([784, 30, 10])\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 2.0\n",
    "mini_batch_size = 10\n",
    "print(f\"Now starting the learning, of {epochs} epochs, this may take a while\")\n",
    "net.SGD(training_data=training_data, epochs=epochs, mini_batch_size=mini_batch_size, eta=learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But how do these external parameters like the learning rate and the structure of the network influence its Classification rate?**\n",
    "\n",
    "In order to visualize the differences, I have ploted 3 different networks with the code below.\n",
    "\n",
    "![alt](../../images/networks_comparison.png)\n",
    "\n",
    "(Note that green has a structure of [784,10], as shown below\n",
    "\n",
    "Note that, as we initialize the network randomly, your results will differ from mine. To give all networks \"the same start\", one would have to extract the weights and biases upon initialization (shallow copying) and then pass it into each new initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting the networks now, this may take a while\n",
      "network data {'sizes': [784, 30, 10], 'epochs': 2, 'mini_batch_size': 10, 'learning_rate': 2.0}\n",
      "Epoch 0 : 9074 / 10000\n",
      "Epoch 1 : 9268 / 10000\n",
      "network data {'sizes': [784, 30, 10], 'epochs': 25, 'mini_batch_size': 10, 'learning_rate': 2.5}\n",
      "Epoch 0 : 8993 / 10000\n",
      "Epoch 1 : 9104 / 10000\n",
      "Epoch 2 : 9203 / 10000\n",
      "Epoch 3 : 9253 / 10000\n",
      "Epoch 4 : 9316 / 10000\n",
      "Epoch 5 : 9320 / 10000\n",
      "Epoch 6 : 9337 / 10000\n",
      "Epoch 7 : 9382 / 10000\n",
      "Epoch 8 : 9395 / 10000\n",
      "Epoch 9 : 9379 / 10000\n",
      "Epoch 10 : 9426 / 10000\n",
      "Epoch 11 : 9412 / 10000\n",
      "Epoch 12 : 9421 / 10000\n",
      "Epoch 13 : 9425 / 10000\n",
      "Epoch 14 : 9464 / 10000\n",
      "Epoch 15 : 9457 / 10000\n",
      "Epoch 16 : 9447 / 10000\n",
      "Epoch 17 : 9450 / 10000\n",
      "Epoch 18 : 9451 / 10000\n",
      "Epoch 19 : 9458 / 10000\n",
      "Epoch 20 : 9465 / 10000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from neuralnets import mnist_loader, network, plot\n",
    "\n",
    "\n",
    "# This is how we define multiple networks for comparison\n",
    "networks = [\n",
    "    {\n",
    "        'sizes': [784, 30, 10],\n",
    "        'epochs': 2,\n",
    "        'mini_batch_size': 10,\n",
    "        'learning_rate': 2.0,\n",
    "    },\n",
    "    {\n",
    "        'sizes': [784, 30, 10],\n",
    "        'epochs': 25,\n",
    "        'mini_batch_size': 10,\n",
    "        'learning_rate': 2.5,\n",
    "    },\n",
    "    {\n",
    "        'sizes': [784, 10],\n",
    "        'epochs': 25,\n",
    "        'mini_batch_size': 10,\n",
    "        'learning_rate': 2.5,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Plotting the networks now, this may take a while\")\n",
    "\n",
    "\n",
    "def plot_networks(networks):\n",
    "    plot_data = []\n",
    "    for network_data in networks:\n",
    "        print(\"network data\", network_data)\n",
    "        # Reloading on each iteration because of EOF issues with training_data\n",
    "        training_data, _, test_data = mnist_loader.load_data_wrapper()\n",
    "        net = network.Network(network_data['sizes'])\n",
    "        cr = net.SGD(training_data=training_data, epochs=network_data['epochs'], mini_batch_size=network_data[\n",
    "                     'mini_batch_size'], eta=network_data['learning_rate'], test_data=test_data)\n",
    "        plot_data.append(\n",
    "            (network_data['epochs'], cr, network_data['learning_rate']))\n",
    "\n",
    "    # Now we plot the classification results of all network\n",
    "    plot.plot_learning_rate(plot_data, xlabel=\"epochs\", ylabel=\"Correct Classifications %\",\n",
    "         title='Training Model output')\n",
    "\n",
    "\n",
    "plot_networks(networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key findings\n",
    "\n",
    "\n",
    "## About the learning rate\n",
    "When thinking of the gradient descent apllied in the `backprop` function in a 3-D space, one can image a ball rolling down to a minimum. Then it becomes aparent why a low learning rate approaches a good classification rate  very slowly while a high rate \"overshoots\" it at times, leading to a slightly lower classification rate.\n",
    "Illustration:\n",
    "\n",
    "![ball](http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png)\n",
    "\n",
    "At the end of the day, all our network is doing at each iteration is not minimize this cost function via Gradient descent\n",
    "\n",
    "$$ΔC≈∇C⋅Δv$$\n",
    "\n",
    "You can read more about it [here](http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent)\n",
    "\n",
    "## About the structure\n",
    "\n",
    "I find it actually surprising that a network without any hidden layers performs so well [784 x 10]. Yet I will still have to dig deeper into why.\n",
    "\n",
    "## About weights and biases\n",
    "\n",
    "It finally became apparent to me what people mean when they speak of networks being trained. When saving the weights and biases and initializing a new network with them it will start where it left of.\n",
    "\n",
    "## About the results\n",
    "\n",
    "I think 95% is already a great start, given the small amount of code used. For comparison: Random results are 10%, counting the darkness levels gets around 50& and Support vector machines get up to 98.5% for the same problem set. \n",
    "As SVMs are far easier to implement, we should definately keep them in mind.\n",
    "Modern CCN Networks get a classification rate as high as 99.979 % for the same dataset. But our network has already performed quite well. \n",
    "\n",
    "After all some of the images are even hard for humans to identify: ![hard_images](http://neuralnetworksanddeeplearning.com/images/mnist_really_bad_images.png)\n",
    "\n",
    "To learn more about how to interpret these results, as well as how the network internally works, I recommend this video:\n",
    "\n",
    "[![3b1bT](http://img.youtube.com/vi/aircAruvnKk/0.jpg)](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "\n",
    "## Closing thoughts\n",
    "\n",
    "A interesting observation here is that:\n",
    "\n",
    "**sophisticated algorithm ≤ simple learning algorithm + good training data**\n",
    "\n",
    "One thought that came to mind is the following:\n",
    "\n",
    "Why couldnt we just calculate differentials and then make adjustments for all the **external parameters** (e.g. learning rate, structure), similarly to how it is done for the internal parameters of weights and biases via the Gradient descent? \n",
    "\n",
    "This way one could theoretically optimize all parameters associated with a network and get the highest classification rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
