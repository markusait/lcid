{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an simplification of our neural network problem. Instead of\n",
    "converting an 28x28 image to a 784 network layer to and let it figure out which number it is, we want the network to learn addition.\n",
    "\n",
    "1. We start with the most easiest case of doing addition, addition with 0.\n",
    "- For learning purposes we create a very simple neural network [1,1,1]. This way core concepts become more apparent. \n",
    "2. Then We let it learn addition with 2 random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need it for vector multiplication\n",
    "import numpy as np\n",
    "\n",
    "class Network_1_D(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Intitalizing  1 dimensional network of size\n",
    "        1 -> 1 -> 1\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # Deterministically\n",
    "        self.biases = [0.2, 0.52]\n",
    "        self.weights = [0.4, 0.7]\n",
    "\n",
    "    def SGD(self, x, y, eta, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using stochastic gradient descent\n",
    "        \"\"\"\n",
    "        for _ in range(epochs):\n",
    "            # Starting input is first layer of network\n",
    "            a = x\n",
    "            # Adjustments for weights and biases\n",
    "            nabla_b = [0, 0]\n",
    "            nabla_w = [0, 0]\n",
    "            activations = [a]\n",
    "            # storing all z vectors layer, by layer for backpropagation\n",
    "            zs = []\n",
    "            # Feedforwarding until we arrive at Final layer\n",
    "            for b, w in zip(self.biases, self.weights):\n",
    "                z = (w * a) + b\n",
    "                zs.append(z)\n",
    "                a = sigmoid(z)\n",
    "                activations.append(a)\n",
    "            # Mathematically the derivative will have a 2 here\n",
    "            # But we can leave it out because we can just adjust the learning rate\n",
    "            delta = (a - y) * sigmoid_prime(zs[-1])  # EQ (30) / (BP1)\n",
    "            delta_C_b = delta  # EQ (BP 3)\n",
    "            delta_C_w = activations[-2] * delta  # EQ (BP4)\n",
    "            # Starting to store all the adjustments from the back\n",
    "            nabla_b[-1] = delta_C_b\n",
    "            nabla_w[-1] = delta_C_w\n",
    "            # Starting backprogation now\n",
    "            for l in range(2, self.num_layers):\n",
    "                z = zs[-l]\n",
    "                w = self.weights[-l+1]\n",
    "                delta = w * delta * sigmoid_prime(z)  # EQ (BP2)\n",
    "                nabla_b[-l] = delta  # EQ (BP3)\n",
    "                nabla_w[-l] = activations[-l-1] * delta  # EQ (BP4)\n",
    "\n",
    "            # Now adjusting weights and biases to deliver a better result next epoch\n",
    "            new_weights = []\n",
    "            for w, nw in zip(self.weights, nabla_w):\n",
    "                new_weight = w - (eta * nw)\n",
    "                new_weights.append(new_weight)\n",
    "\n",
    "            new_biases = []\n",
    "            for b, nb in zip(self.biases, nabla_b):\n",
    "                new_bias = b - (eta * nb)\n",
    "                new_biases.append(new_bias)\n",
    "            self.weights = new_weights\n",
    "            self.biases = new_biases\n",
    "            # Cost of the network is defined as C = (a - y) ** 2\n",
    "            # print(\"w\", self.weights, \"b\", self.biases)\n",
    "            # print(\"a\", a, \"Cost\", Cost)\n",
    "        return a\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are comming very close to 1. However this approach comes with a number of meaningful asterixes. \n",
    "And should not be considered sophisticated but rather as a good learning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result 0.9590591854736482\n"
     ]
    }
   ],
   "source": [
    "net = Network_1_D([1, 1, 1])\n",
    "training_data = 1\n",
    "excpected_output = 1\n",
    "learning_rate = 1\n",
    "print(\"result\", net.SGD(x=training_data, y=excpected_output, eta=learning_rate, epochs=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Addition with 2 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training network\n",
      "correct results 0.32550000000000007\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from neuralnets import network\n",
    "import numpy as np\n",
    "\n",
    "def vectorized_result(j, l):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((l, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def add_two_numbers():\n",
    "    td_size = 1000\n",
    "    num_arrays = [np.random.choice(range(10), replace=True, size=(2, 1)) for x in range(td_size)]\n",
    "    # Training data has the shape [(2,1),(19,1)], as the second item is the sum represented in array form\n",
    "    training_data = [(num_array, vectorized_result(j=np.sum(num_array), l=19)) for num_array in num_arrays[:td_size-100]]\n",
    "    # Note how we are not vectoring but just return the summation\n",
    "    # Test data has the shape (2,1), 1, as the second item is just the number 0-18 of the sum\n",
    "    test_data = [(num_array, np.sum(num_array)) for num_array in num_arrays[td_size-100:]]\n",
    "    net = network.Network([2, 19, 19])\n",
    "    print(\"training network\")\n",
    "    \n",
    "    print(\"correct results\", net.SGD(training_data=training_data, test_data=test_data,eta=1, epochs=100, mini_batch_size=10, should_print=False, mean=True))\n",
    "    \n",
    "add_two_numbers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results vary around 30% and I am sure with a different structure one should be able to get it up to 99%. There are many different approaches one can take in representing these numbers (e.g binary). \n",
    "\n",
    "Afterall note that by using perceptrons one can very easly train a network to learn addition. \n",
    "\n",
    "[More info here](http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
